{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Evaluating the LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "from llm import getChatChain\n",
    "from app import load_documents_into_database\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que avalia a Precisão e Accuracy do Modelo LLM\n",
    "def evaluate(llm_model_name: str, db: Chroma) -> tuple:\n",
    "    accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "        Score 1: The answer is completely irrelevant or incoherent in relation to the reference.\n",
    "        Score 2: The answer is mostly irrelevant, with few or no correct parts.\n",
    "        Score 3: The answer has some relevance but is mostly incorrect or out of context.\n",
    "        Score 4: The answer has moderate relevance but contains several significant inaccuracies.\n",
    "        Score 5: The answer has moderate relevance but contains some notable inaccuracies.\n",
    "        Score 6: The answer is generally correct but contains a reasonable number of minor errors or omissions.\n",
    "        Score 7: The answer is mostly correct and relevant but contains some minor errors or omissions.\n",
    "        Score 8: The answer is very correct and relevant, with only small inaccuracies or omissions.\n",
    "        Score 9: The answer is almost entirely accurate and relevant, with only one or two small inaccuracies or omissions.\n",
    "        Score 10: The answer is completely accurate and perfectly aligns with the reference, with no errors or omissions.\"\"\"\n",
    "    }\n",
    "\n",
    "    evaluator = load_evaluator(\n",
    "        \"labeled_score_string\",\n",
    "        criteria=accuracy_criteria,\n",
    "        llm=Ollama(model=llm_model_name),\n",
    "    )\n",
    "\n",
    "    chat = getChatChain(Ollama(model=llm_model_name), db)\n",
    "    df = pd.read_csv(\"evaluate.csv\")\n",
    "    print(\"\\n[INFO] Evaluating model: \", llm_model_name)\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        reference_answer = row['answer']\n",
    "        model_answer = chat(question=question)\n",
    "        try:\n",
    "            evaluation = evaluator.evaluate_strings(\n",
    "                prediction=model_answer,\n",
    "                reference=reference_answer,\n",
    "                input=question\n",
    "            )\n",
    "            print(evaluation)\n",
    "            # print(\"\\n[QUESTION] \" + evaluation['reasoning'],evaluation['score'])\n",
    "        except ValueError as e:\n",
    "            print(\"\\n[EXCEPTION] \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 47.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Evaluating model:  mistral\n",
      " I apologize for the confusion, but there seems to be a mistake in the question as there's no Article 236 mentioned in the provided text from the Portuguese Criminal Code. The text only discusses Articles 2034, 2037, 2036, and 69-b, as well as Article 374-b indirectly. If you meant to ask about one of these articles, please let me know which one!\n",
      "\n",
      "If you'd like to know the content of Articles 2034, 2037, 2036, and 69-b:\n",
      "\n",
      "Article 2034 deals with the disqualification for succession due to certain criminal offenses against the author of a succession or their relatives (conjugue, descendants, ascendants, adopters, or adopted persons) within the terms and effects set out in paragraph a) of the article. It also specifies that this does not affect the provisions of Article 2036.\n",
      "\n",
      "Article 2037 discusses the effects of this disqualification for succession.\n",
      "\n",
      "Article 2036 allows for certain exceptions to the disqualification for succession.\n",
      "\n",
      "Article 69-b prohibits those who have been convicted of crimes against autodetermination sexual and sexual freedom from exercising public functions, starting from March 1, 2024.\n",
      "[EXCEPTION] \n",
      "O Modelo demorou 50.79 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "#Avaliação do Mistral segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"mistral\",\"nomic-embed-text\",\"../Evaluation_Temp\",True)\n",
    "evaluate(\"mistral\",db)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 97.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Evaluating model:  llama2\n",
      "Based on the provided research documents, Article 236 of the Portuguese Penal Code states that \"Whoever, publicly and repeatedly, incites hatred against a people, intending to trigger a war, is punished with imprisonment from 6 months to 3 years.\"\n",
      "[EXCEPTION] \n",
      "O Modelo demorou 37.46 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "#Avaliação do Llama2 segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"llama2\",\"nomic-embed-text\",\"../Evaluation_Temp\",True)\n",
    "evaluate(\"llama2\",db)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 103.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Evaluating model:  zephyr\n",
      "Unfortunately, based on the provided context, it is unclear which article the speaker is referring to as \"the other\". The given conversation only mentions Articles 2034, 2037, and 2036 indirectly, as well as Article 69-b. Without further information or context, it is impossible to determine whether the speaker is referring to Article 236 of the Portuguese Criminal Code or another article altogether. Please provide more details or clarify which article is being referred to!\n",
      "[EXCEPTION] \n",
      "O Modelo demorou 65.33 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "#Avaliação do Zephyr segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"zephyr\",\"nomic-embed-text\",\"../Evaluation_Temp\",True)\n",
    "evaluate(\"zephyr\",db)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
