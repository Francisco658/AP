{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Evaluating the LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "from llm import getChatChain\n",
    "from app import load_documents_into_database\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que avalia a Precisão e Accuracy do Modelo LLM\n",
    "def evaluate(llm_model_name: str, db: Chroma, inicio: float) -> tuple:\n",
    "    accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "        Score 1: The answer is completely irrelevant or incoherent in relation to the reference.\n",
    "        Score 2: The answer is mostly irrelevant, with few or no correct parts.\n",
    "        Score 3: The answer has some relevance but is mostly incorrect or out of context.\n",
    "        Score 4: The answer has moderate relevance but contains several significant inaccuracies.\n",
    "        Score 5: The answer has moderate relevance but contains some notable inaccuracies.\n",
    "        Score 6: The answer is generally correct but contains a reasonable number of minor errors or omissions.\n",
    "        Score 7: The answer is mostly correct and relevant but contains some minor errors or omissions.\n",
    "        Score 8: The answer is very correct and relevant, with only small inaccuracies or omissions.\n",
    "        Score 9: The answer is almost entirely accurate and relevant, with only one or two small inaccuracies or omissions.\n",
    "        Score 10: The answer is completely accurate and perfectly aligns with the reference, with no errors or omissions.\"\"\"\n",
    "    }\n",
    "\n",
    "    evaluator = load_evaluator(\n",
    "        \"labeled_score_string\",\n",
    "        criteria=accuracy_criteria,\n",
    "        llm=Ollama(model=llm_model_name),\n",
    "    )\n",
    "\n",
    "    chat = getChatChain(Ollama(model=llm_model_name), db)\n",
    "    df = pd.read_csv(\"evaluate.csv\")\n",
    "    print(\"\\n[INFO] Evaluating model: \", llm_model_name)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        reference_answer = row['answer']\n",
    "        model_answer = chat(question=question)\n",
    "        try:\n",
    "            evaluation = evaluator.evaluate_strings(\n",
    "                prediction=model_answer,\n",
    "                reference=reference_answer,\n",
    "                input=question\n",
    "            )\n",
    "            print(evaluation)\n",
    "            \n",
    "            # Save results to CSV\n",
    "            with open(\"Stats.csv\", \"a\") as f:\n",
    "                f.write(f\"{llm_model_name},{evaluation['accuracy']},{time.time() - inicio}\\n\")\n",
    "                print(\"\\n[QUESTION] \" + evaluation['reasoning'], evaluation['accuracy'])\n",
    "        except ValueError as e:\n",
    "            print(\"\\n[EXCEPTION] \", e)\n",
    "\n",
    "# Ensure the CSV has the correct header\n",
    "with open(\"Stats.csv\", \"w\") as f:\n",
    "    f.write(\"model,score,time\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliação do Mistral segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"mistral\",\"nomic-embed-text\",\"../Final PDF Files\",True)\n",
    "evaluate(\"mistral\",db,inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliação do Llama2 segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"llama2\",\"nomic-embed-text\",\"../Final PDF Files\",True)\n",
    "evaluate(\"llama2\",db,inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliação do Zephyr segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"zephyr\",\"nomic-embed-text\",\"../Final PDF Files\",True)\n",
    "evaluate(\"zephyr\",db,inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
