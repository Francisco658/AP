{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Evaluating the LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "from llm import getChatChain\n",
    "from app import load_documents_into_database\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que avalia a Precisão e Accuracy do Modelo LLM\n",
    "def evaluate(llm_model_name: str, db: Chroma, inicio: float) -> tuple:\n",
    "    accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "        Score 1: The answer is completely irrelevant or incoherent in relation to the reference.\n",
    "        Score 2: The answer is mostly irrelevant, with few or no correct parts.\n",
    "        Score 3: The answer has some relevance but is mostly incorrect or out of context.\n",
    "        Score 4: The answer has moderate relevance but contains several significant inaccuracies.\n",
    "        Score 5: The answer has moderate relevance but contains some notable inaccuracies.\n",
    "        Score 6: The answer is generally correct but contains a reasonable number of minor errors or omissions.\n",
    "        Score 7: The answer is mostly correct and relevant but contains some minor errors or omissions.\n",
    "        Score 8: The answer is very correct and relevant, with only small inaccuracies or omissions.\n",
    "        Score 9: The answer is almost entirely accurate and relevant, with only one or two small inaccuracies or omissions.\n",
    "        Score 10: The answer is completely accurate and perfectly aligns with the reference, with no errors or omissions.\"\"\"\n",
    "    }\n",
    "\n",
    "    evaluator = load_evaluator(\n",
    "        \"labeled_score_string\",\n",
    "        criteria=accuracy_criteria,\n",
    "        llm=Ollama(model=llm_model_name),\n",
    "    )\n",
    "\n",
    "    chat = getChatChain(Ollama(model=llm_model_name), db)\n",
    "    df = pd.read_csv(\"evaluate.csv\")\n",
    "    print(\"\\n[INFO] Evaluating model: \", llm_model_name)\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        reference_answer = row['answer']\n",
    "        model_answer = chat(question=question)\n",
    "        try:\n",
    "            evaluation = evaluator.evaluate_strings(\n",
    "                prediction=model_answer,\n",
    "                reference=reference_answer,\n",
    "                input=question\n",
    "            )\n",
    "            print(evaluation)\n",
    "            \n",
    "            # Save results to CSV\n",
    "            with open(\"Stats.csv\", \"a\") as f:\n",
    "                f.write(f\"{llm_model_name},{evaluation['score']},{time.time() - inicio}\\n\")\n",
    "                print(\"\\n[QUESTION] \" + evaluation['reasoning'],evaluation['score'])\n",
    "        except ValueError as e:\n",
    "            print(\"\\n[EXCEPTION] \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1156.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n",
      "\n",
      "[INFO] Evaluating model:  mistral\n",
      " The human chest has three parts: upper, middle, and lower. (Sources: Training\\_Muscles.pdf)\n",
      "[EXCEPTION] \n",
      "O Modelo demorou 46.08 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "#Avaliação do Mistral segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"mistral\",\"nomic-embed-text\",\"../Final PDF Files\",True)\n",
    "evaluate(\"mistral\",db,inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1074.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n",
      "\n",
      "[INFO] Evaluating model:  llama2\n",
      "Based on the provided research documents, the answer to the question \"How many parts are there in the human chest?\" is:\n",
      "\n",
      "There are 4 parts to the human chest:\n",
      "\n",
      "1. Pectoralis major muscle\n",
      "2. Pectoralis minor muscle\n",
      "3. Ribcage (made up of 12 ribs)\n",
      "4. Sternum (or breastbone)\n",
      "[EXCEPTION] \n",
      "O Modelo demorou 59.76 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "#Avaliação do Llama2 segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"llama2\",\"nomic-embed-text\",\"../Final PDF Files\",True)\n",
    "evaluate(\"llama2\",db,inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 938.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n",
      "\n",
      "[INFO] Evaluating model:  zephyr\n",
      "The human chest is composed of three parts: upper, middle, and lower, according to the provided research document \"Training_Muscles.pdf\". The middle and lower portions make up 80% of the chest mass, so it makes sense to focus on working these parts with more sets in flat bench presses/flyes than incline ones for optimal training.\n",
      "[EXCEPTION] \n",
      "O Modelo demorou 59.01 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "#Avaliação do Zephyr segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"zephyr\",\"nomic-embed-text\",\"../Final PDF Files\",True)\n",
    "evaluate(\"zephyr\",db,inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
