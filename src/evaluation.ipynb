{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Evaluating the LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "from llm import getChatChain\n",
    "from app import load_documents_into_database\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que avalia a Precisão e Accuracy do Modelo LLM\n",
    "def evaluate(llm_model_name: str, db: Chroma, inicio: float) -> tuple:\n",
    "    accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "        Score 1: The answer is completely irrelevant or incoherent in relation to the reference.\n",
    "        Score 2: The answer is mostly irrelevant, with few or no correct parts.\n",
    "        Score 3: The answer has some relevance but is mostly incorrect or out of context.\n",
    "        Score 4: The answer has moderate relevance but contains several significant inaccuracies.\n",
    "        Score 5: The answer has moderate relevance but contains some notable inaccuracies.\n",
    "        Score 6: The answer is generally correct but contains a reasonable number of minor errors or omissions.\n",
    "        Score 7: The answer is mostly correct and relevant but contains some minor errors or omissions.\n",
    "        Score 8: The answer is very correct and relevant, with only small inaccuracies or omissions.\n",
    "        Score 9: The answer is almost entirely accurate and relevant, with only one or two small inaccuracies or omissions.\n",
    "        Score 10: The answer is completely accurate and perfectly aligns with the reference, with no errors or omissions.\"\"\"\n",
    "    }\n",
    "\n",
    "    evaluator = load_evaluator(\n",
    "        \"labeled_score_string\",\n",
    "        criteria=accuracy_criteria,\n",
    "        llm=Ollama(model=llm_model_name),\n",
    "    )\n",
    "\n",
    "    chat = getChatChain(Ollama(model=llm_model_name), db)\n",
    "    df = pd.read_csv(\"evaluate.csv\")\n",
    "    f = open(\"Stats.csv\", \"a\")\n",
    "    print(\"\\n[INFO] Evaluating model: \", llm_model_name)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        reference_answer = row['answer']\n",
    "        model_answer = chat(question=question)\n",
    "        try:\n",
    "            evaluation = evaluator.evaluate_strings(\n",
    "                prediction=model_answer,\n",
    "                reference=reference_answer,\n",
    "                input=question\n",
    "            )\n",
    "            score = evaluation.get('score', '')\n",
    "            print(evaluation)\n",
    "            \n",
    "            f.write(f\"{llm_model_name},{score},{time.time() - inicio}\\n\")\n",
    "            print(\"\\n[QUESTION] \" + evaluation.get('reasoning', ''), score)\n",
    "        except ValueError as e:\n",
    "            print(\"\\n[EXCEPTION] \", str(e))\n",
    "            f.write(f\"{llm_model_name},{score},{time.time() - inicio}\\n\")\n",
    "\n",
    "# Ensure the CSV has the correct header\n",
    "with open(\"Stats.csv\", \"w\") as f:\n",
    "    f.write(\"model,score,time\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1100.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n",
      "\n",
      "[INFO] Evaluating model:  mistral\n",
      " I cannot answer your question with the provided research. The research focuses on the benefits of an exercise method for promoting joint stability and balanced muscular development, and does not mention anything about the number of parts in the human chest.\n",
      "[EXCEPTION]  Invalid output:  Based on the information provided in the user question and the assistant's response, I would rate the quality of the assistant's response as follows:\n",
      "\n",
      "Explanation: The assistant correctly identified that the given research does not provide an answer to the user question. However, it could have mentioned that the human chest is typically divided into three parts: the upper chest, middle chest, and lower chest. This information would have been helpful for the user. Therefore, the response was relevant but mostly incorrect in providing a complete answer.\n",
      "\n",
      "Rating: 4.\n",
      "\n",
      "The assistant's response did acknowledge that it could not provide an accurate answer based on the given research. However, it could have provided some additional relevant information about the human chest to help the user.. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "O Modelo demorou 51.96 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "# Avaliação do Mistral segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"mistral\", \"nomic-embed-text\", \"../Final PDF Files\", True)\n",
    "evaluate(\"mistral\", db, inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio), 2)) + \" segundos a gerar as respostas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1081.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n",
      "\n",
      "[INFO] Evaluating model:  llama2\n",
      "Based on the provided research documents, there are three parts to the human chest: upper, middle, and lower. According to the \"Training Muscles\" document, these parts make up 80% of the chest mass, and it is recommended to focus on working these parts with more sets in flat bench presses/flyes than incline ones.\n",
      "[EXCEPTION]  Invalid output: Rating: [8]\n",
      "\n",
      "In my evaluation, the response provided by the AI assistant is generally correct and relevant to the question asked. The assistant accurately identified the three parts of the human chest (upper, middle, and lower) based on the provided research documents. The answer is well-structured and easy to understand, with clear explanations for each part of the chest.\n",
      "\n",
      "However, I would deduct a point for minor inaccuracies or omissions. For instance, the assistant did not provide any information about the size or muscles of each part of the chest, which could be useful contextual information for the user. Additionally, the assistant did not provide any references or citations to support their claims, which is an important aspect of academic integrity and credibility.\n",
      "\n",
      "Overall, while the response is mostly accurate and relevant, there is room for improvement in terms of providing more detailed information and proper citation of sources.. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "O Modelo demorou 61.4 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "#Avaliação do Llama2 segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"llama2\",\"nomic-embed-text\",\"../Final PDF Files\",True)\n",
    "evaluate(\"llama2\",db,inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n",
      "Loading .pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .md files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1059.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and loading documents into Chroma\n",
      "\n",
      "[INFO] Evaluating model:  zephyr\n",
      "The provided research documents mention three parts that make up the human chest:\n",
      "\n",
      "1. Grains group: Whole grains such as brown rice and oats are included in this group. These foods provide carbohydrates, fiber, and other important nutrients. (Source: ../Final PDF Files/Nutrition_Facts.pdf, Page 1)\n",
      "\n",
      "2. Meat, Fish, and Beans group: This group includes both animal and plant-based sources of protein such as chicken, fish, beans, and lentils. (Source: ../Final PDF Files/Nutrition_Facts.pdf, Page 1)\n",
      "\n",
      "3. Milk group: Low-fat cheese is a part of this group that provides calcium, protein, and other essential nutrients. (Source: ../Final PDF Files/Nutrition_Facts.pdf, Page 1)\n",
      "\n",
      "Note: The provided research documents do not explicitly mention the three parts of the human chest. This question seems to be unrelated to the given context. However, if you are asking about the respiratory system, then the answer would be:\n",
      "\n",
      "1. Upper chest (thoracic cage): This part includes the ribs and breastbone. The muscles between the ribs help in breathing by expanding the chest cavity during inhalation.\n",
      "\n",
      "2. Middle chest (diaphragm): Located below the lungs, this dome-shaped muscle helps in breathing by contracting during exhalation and relaxing during inhalation.\n",
      "\n",
      "3. Lower chest: This part is not separate from the upper chest but helps in breathing along with it. It includes the lower ribs and muscles attached to them that help expand the chest cavity during inhalation.\n",
      "[EXCEPTION]  Invalid output: Explanation: The given response is not directly related to the question asked. The provided information pertains to the nutrition facts, which does not provide any insight into the anatomy of the human chest. Therefore, based on the criteria mentioned, we can rate the response as follows:\n",
      "\n",
      "Rating: 3 (The answer has some relevance but is mostly incorrect or out of context). Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "O Modelo demorou 74.86 segundos a gerar as respostas.\n"
     ]
    }
   ],
   "source": [
    "#Avaliação do Zephyr segundo o Tempo, a Precisão e a Accuracy.\n",
    "inicio = time.time()\n",
    "db = load_documents_into_database(\"zephyr\",\"nomic-embed-text\",\"../Final PDF Files\",True)\n",
    "evaluate(\"zephyr\",db,inicio)\n",
    "fim = time.time()\n",
    "print(\"O Modelo demorou \" + str(round((fim-inicio),2)) + \" segundos a gerar as respostas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
